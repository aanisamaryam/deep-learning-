!pip install -U datasets
%%capture
!pip install transformers==4.19.2
!pip install rouge_score
from datasets import load_metric
import pandas as pd
df = pd.read_csv("/content/drive/MyDrive/code cycle/wikiHow.csv")
print(df.shape)
df = df.dropna()
print(df.shape)
print(df.shape)
df = df.drop_duplicates()
print(df.shape)
df['length'] = df.paragraph.map(lambda x : len(x.split(" ")))
num_of_words = df.length
from matplotlib import pyplot as plt

fig = plt.figure(figsize = (5,3))
plt.hist(num_of_words.to_numpy() , bins = [0,50,100,200,300,500,1000])
plt.title("WORD COUNT DISTRIBUTION")
plt.show()
tempDf = df[df.length <= 200]
tempDf.shape

# # preprocessing the input data such as defining thhe max_length of both inputs
# and outputs then padding then tokenisation then ignoring padding during loss
# calculation

max_input_length = 1024
max_output_length = 64
batch_size = 16

def process_data_to_model_inputs(batch):
    # tokenize the inputs and labels
    inputs = tokenizer( batch['paragraph'], padding = 'max_length', truncation = True, max_length = max_input_length)

    outputs = tokenizer( batch['heading'], padding = 'max_length', truncation = True, max_length = max_output_length)


    batch['input_ids'] = inputs.input_ids
    batch['attention_mask'] = inputs.attention_mask

    batch['global_attention_mask'] = len(batch['input_ids']) * [[0 for _ in range(len(batch['input_ids'][0]))] ]

    batch['global_attention_mask'][0][0] = 1
    batch['labels'] = outputs.input_ids

 # ignoring the PAD token
    batch['labels'] = [ [ -100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch['labels']]
    return batch

# splitting the data into training set validation set and testing set
import numpy as np
train, validate,test = np.split(tempDf.sample(frac=1, random_state=42), [int(.6*len(tempDf)), int(.7*len(tempDf))])
print(train.shape)
print(validate.shape)
print(test.shape)
validate = validate[:20]
validate.shape

from datasets import Dataset
train_dataset = Dataset.from_pandas(train)
##validate_dataset = Dataset.from_pandas(validate)
train_dataset = train_dataset.map(process_data_to_model_inputs, batched = True, batch_size = batch_size,remove_columns = ['title','heading','paragraph','length','__index_level_0__'])  
train_dataset.set_format(type = 'torch', columns = ['input_ids','attention_mask','global_attention_mask','labels'])

from datasets import Dataset
##train_dataset = Dataset.from_pandas(train)
validate_dataset = Dataset.from_pandas(validate)
validate_dataset = validate_dataset.map(process_data_to_model_inputs, batched = True, batch_size = batch_size,remove_columns = ['title','heading','paragraph','length','__index_level_0__'])  # Changed train_dataset to validate_dataset
validate_dataset.set_format(type = 'torch', columns = ['input_ids','attention_mask','global_attention_mask','labels'])

from transformers import AutoModelForSeq2SeqLM
led = AutoModelForSeq2SeqLM.from_pretrained("allenai/led-base-16384", gradient_checkpointing=True, use_cache = False)
led.config.num_beams = 2
led.config.max_length = 64
led.config.min_length = 2
led.config.length_penalty = 2.0
led.config.early_stopping = True
led.config.no_repeat_ngram_size = 3
rouge = load_metric("rouge")

def comput_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions

    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens = True)
    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens = True)

    rouge_output = rouge.compute(predictions = pred_str, references = label_str, rouge_types = ["rouge2"])["rouge2"].mid

    return{"rouge_precision": round(rouge_output.precision, 4), "rouge_recall": round(rouge_output.recall, 4), "rouge_fmeasure": round(rouge_output.fmeasure, 4)}
from transformers import Seq2SeqTrainer,Seq2SeqTrainingArguments
import transformers
transformers.logging.set_verbosity_info()
training_args = Seq2SeqTrainingArguments(
    predict_with_generate=True,
    evaluation_strategy="steps",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    output_dir="./",
    logging_steps=5,
    eval_steps=10,
    save_steps=10,
    save_total_limit=2,
    gradient_accumulation_steps=4,
    num_train_epochs=10)

trainer = Seq2SeqTrainer(
    model=led,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=validate_dataset,
    compute_metrics=comput_metrics)
trainer.train()

import pandas as pd
sample_paragraph = "The reason why I loved the top-down culture at Apple is that important decisions are taken faster. Having an expert giving you green light or not keeps the momentum. How many times in a bottom-up culture do we spend weeks and weeks, sometimes even months, trying to get alignment with +10 people, because every single person needs to agree with the point of view? It is exhausting. So again, my experience is that having that one leader to look up to to help guide decisions is time-saving, it helps us focus on the design craft, instead of project management"
data = [sample_paragraph]
df = pd.DataFrame(data , columns=['paragraph'])
df['paragraph'][0]
from datasets import Dataset
df_test = Dataset.from_pandas(df)
df_test

from datasets import load_metric
import torch

from datasets import load_dataset,load_metric
from transformers import LEDTokenizer,LEDForConditionalGeneration

tokenizer = LEDTokenizer.from_pretrained('/content/checkpoint-40')
model = LEDForConditionalGeneration.from_pretrained('/content/checkpoint-40').to("cuda").half()

def generate_answer (batch):
  inputs_dicts = tokenizer(batch["paragraph"], max_length=512, padding="max_length", truncation=True, return_tensors="pt")
  input_ids = inputs_dicts.input_ids.to("cuda")
  attention_mask = inputs_dicts.attention_mask.to("cuda")
  global_attention_mask = torch.zeros_like(attention_mask)

  predicted_abstract_ids = model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)
  batch["generated_heading"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)
  return batch

result = df_test.map(generate_answer, batched=True, batch_size=1)
print(result)
result['generated_heading']


  
