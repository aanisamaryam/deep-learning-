import requests  # to retrieve HTML content from web pages
from bs4 import BeautifulSoup  # BeautifulSoup is used for extracting specific elements from HTML
import re  # pattern matching and text manipulation
import csv  # used to save extracted data into CSV files for further analysis or storage

for count in range(4000):
  url = "https://www.wikihow.com/Special:Randomizer"

  # Send an HTTP request to receive the contents
  response = requests.get(url)
  html_content = response.content

  # Parsing (analyzing and processing) the HTML code that we retrieved using BeautifulSoup
  soup = BeautifulSoup(html_content, 'html.parser')  # Corrected the typo here
  article_title = soup.find('title').text.strip()
  ##print(article_title)

  ## extracting subheadings and paras via html tags
  subheadings = []
  paragraphs = []
  steps = soup.find_all('div', {'class': 'step'})
  for step in steps:
    subheading_element = step.find('b')
    if(subheading_element is not None):
      subheading_text = subheading_element.text.strip().replace('\n','')
      subheading_text = subheading_text.encode('ascii',errors = 'ignore').decode()
      subheading_text = re.sub(r'' , '' ,subheading_text)
      ##print("subheading")
      #print(subheading_text)
      subheadings.append(subheading_text)

      ## making changes in the obtained html code
      subheading_element.extract()
      for span_tag in step.find_all('span'):
        span_tag.extract()

      paragraph_text = step.text.strip().replace('\n','').replace(' ', ' ')
      paragraph_text = paragraph_text.encode('ascii',errors = 'ignore').decode()
      paragraph_text = re.sub(r'' , '' ,paragraph_text)
      ##print("paragraph")
      ##print(paragraph_text)
      paragraphs.append(paragraph_text)

  ## write the contents into a csv file
  if(len(subheadings)):
    with open('wikiHow.csv', mode = 'a', newline='', encoding = 'utf-8') as csv_file:
      writer = csv.writer(csv_file)
      for i in range(len(subheadings)):
        writer.writerow([article_title,subheadings[i],paragraphs[i]])
